import streamlit as st
import os
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø©
st.set_page_config(page_title="SLAA AI Sponsor", page_icon="ğŸ›¡ï¸")
st.title("ğŸ›¡ï¸ Ø±ÙÙŠÙ‚ Ø§Ù„ØªØ¹Ø§ÙÙŠ (Ù†Ø³Ø®Ø© Ø§Ù„ÙˆÙŠØ¨ Ø§Ù„Ø³Ø±ÙŠØ¹Ø©)")

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…ÙØªØ§Ø­ API
if "GROQ_API_KEY" in st.secrets:
    groq_api_key = st.secrets["GROQ_API_KEY"]
else:
    st.error("Ù…ÙØªØ§Ø­ API ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯! ÙŠØ±Ø¬Ù‰ Ø¥Ø¶Ø§ÙØªÙ‡ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Streamlit.")
    st.stop()

@st.cache_resource
def load_library():
    # ÙÙŠ Ø§Ù„Ø³ÙŠØ±ÙØ±ØŒ Ø§Ù„Ù…Ù„ÙØ§Øª Ø³ØªÙƒÙˆÙ† ÙÙŠ Ù†ÙØ³ Ø§Ù„Ù…Ø³Ø§Ø±
    folder_path = "library" 
    
    if not os.path.exists(folder_path):
        os.makedirs(folder_path) # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ù„Ùˆ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
        return None
        
    if not os.listdir(folder_path):
        return "EMPTY"

    with st.spinner("Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©... (ÙŠØ­Ø¯Ø« Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©)"):
        loader = PyPDFDirectoryLoader(folder_path)
        docs = loader.load()
        # ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = splitter.split_documents(docs)
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙˆØ¯ÙŠÙ„ ÙÙ‡Ø±Ø³Ø© Ù…Ø¬Ø§Ù†ÙŠ ÙˆØ³Ø±ÙŠØ¹ ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ CPU Ø§Ù„Ø³ÙŠØ±ÙØ±
        embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        
        vectorstore = FAISS.from_documents(splits, embeddings)
        return vectorstore

vectorstore = load_library()

if not vectorstore or vectorstore == "EMPTY":
    st.warning("âš ï¸ Ø§Ù„Ù…ÙƒØªØ¨Ø© ÙØ§Ø±ØºØ© Ø£Ùˆ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©. ØªØ£ÙƒØ¯ Ù…Ù† Ø±ÙØ¹ Ù…Ø¬Ù„Ø¯ 'library' Ù…Ø¹ Ø§Ù„ÙƒÙˆØ¯.")
    st.stop()

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø§Ù„ØµØ§Ø±Ù…
system_prompt = (
    "You are an expert SLAA Sponsor using the Llama-3-70b model. "
    "Your Goal: Guide the user to sobriety based strictly on the provided context.\n"
    "RULES:\n"
    "1. **ARABIC ONLY**: Reply in Arabic language only.\n"
    "2. **Strict & Wise**: Be compassionate but firm about steps.\n"
    "3. **Context-Driven**: Answer based on the book extracts below.\n"
    "4. **Action**: End with a practical step.\n\n"
    "Context: {context}"
)

prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{input}"),
])

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­Ø±Ùƒ Groq Ø§Ù„Ø®Ø§Ø±Ù‚ (Llama 3 - 70B)
llm = ChatGroq(
    groq_api_key=groq_api_key, 
    model_name="llama3-70b-8192"
)

retriever = vectorstore.as_retriever()
chain = create_retrieval_chain(retriever, create_stuff_documents_chain(llm, prompt))

# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø´Ø§Øª
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if user_input := st.chat_input("ØªØ­Ø¯Ø« Ù…Ø¹ Ù…ÙˆØ¬Ù‡Ùƒ..."):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Llama 3..."):
            response = chain.invoke({"input": user_input})
            st.markdown(response["answer"])
    
    st.session_state.messages.append({"role": "assistant", "content": response["answer"]})