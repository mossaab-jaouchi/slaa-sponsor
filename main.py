import streamlit as st
import os

# 1. Ø£ÙˆØ§Ù…Ø± ØµØ§Ø±Ù…Ø© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹ Ù„Ù„Ø§Ø³ØªØ¶Ø§ÙØ© Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠØ©)
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

st.set_page_config(page_title="SLAA AI Sponsor", page_icon="ğŸ›¡ï¸")
st.title("ğŸ›¡ï¸ Ø±ÙÙŠÙ‚ Ø§Ù„ØªØ¹Ø§ÙÙŠ")

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø¨Ø­Ø°Ø±
try:
    from langchain_community.document_loaders import PyPDFDirectoryLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.embeddings import FastEmbedEmbeddings
    from langchain_community.vectorstores import FAISS
    from langchain_groq import ChatGroq
    from langchain_core.prompts import ChatPromptTemplate
    from langchain.chains import create_retrieval_chain
    from langchain.chains.combine_documents import create_stuff_documents_chain
except ImportError as e:
    st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª: {e}")
    st.stop()

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ÙØªØ§Ø­
if "GROQ_API_KEY" in st.secrets:
    groq_api_key = st.secrets["GROQ_API_KEY"]
else:
    st.warning("âš ï¸ Ø§Ù„Ù…ÙØªØ§Ø­ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.")
    st.stop()

@st.cache_resource
def load_library():
    folder_path = "library"
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
        return None
    
    if not os.listdir(folder_path):
        return "EMPTY"

    try:
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ÙƒØªØ¨ (ÙˆØ¶Ø¹ ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©)..."):
            loader = PyPDFDirectoryLoader(folder_path)
            docs = loader.load()
            
            if not docs:
                return "EMPTY"

            # 2. ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ù‚Ø·Ø¹ Ù…Ù† 1000 Ø¥Ù„Ù‰ 500 Ù„ØªØ®ÙÙŠÙ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø§Ù…
            splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
            splits = splitter.split_documents(docs)
            
            # 3. Ø¥Ø¬Ø¨Ø§Ø± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…Ù„ Ø¨Ø®ÙŠØ· ÙˆØ§Ø­Ø¯ (threads=1)
            embeddings = FastEmbedEmbeddings(
                model_name="BAAI/bge-base-en-v1.5",
                threads=1
            )
            
            vectorstore = FAISS.from_documents(splits, embeddings)
            return vectorstore
            
    except Exception as e:
        st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ (ØºØ§Ù„Ø¨Ø§Ù‹ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ù…ØªÙ„Ø¦Ø©): {e}")
        return None

vectorstore = load_library()

if vectorstore is None:
    st.info("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ù…Ù„ÙØ§Øª PDF.")
    st.stop()
elif vectorstore == "EMPTY":
    st.warning("âš ï¸ Ø§Ù„Ù…ÙƒØªØ¨Ø© ÙØ§Ø±ØºØ©.")
    st.stop()

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙˆØ¬Ù‡
system_prompt = (
    "Answer in Arabic only. You are a strict SLAA sponsor. "
    "Use the context below to guide the user.\n\n"
    "Context: {context}"
)

prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{input}"),
])

llm = ChatGroq(groq_api_key=groq_api_key, model_name="llama3-70b-8192")
retriever = vectorstore.as_retriever()
chain = create_retrieval_chain(retriever, create_stuff_documents_chain(llm, prompt))

if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if user_input := st.chat_input("ØªØ­Ø¯Ø« Ù…Ø¹ Ù…ÙˆØ¬Ù‡Ùƒ..."):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)
    with st.chat_message("assistant"):
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø§ØªØµØ§Ù„..."):
            response = chain.invoke({"input": user_input})
            st.markdown(response["answer"])
    st.session_state.messages.append({"role": "assistant", "content": response["answer"]})